<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <!-- From here: http://stackoverflow.com/a/4389976 -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <title>Entropy Demo</title>
    <script src="entropy-demo.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha384-dq1/gEHSxPZQ7DdrM82ID4YVol9BYyU7GbWlIwnwyPzotpoc57wDw/guX8EaYGPx" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="style.css" type="text/css">
  </head>
  <body>
    <div class="demo-container">
      <div class="demo-description">
        <h2>
          Entropy Demo
        </h2>
        <p>
          I've read many articles online with differing, vague, descriptions of entropy. But I recently found out that it has a clear mathematical representation!
        </p>

        <p>
          Suppose you're performing an experiment which has 5 possible outcomes. We'll label them \(i = {1, ..., 5}\). And suppose that they each occur with probability \(p(i)\). \(p\) is called a distribution, which is just a fancy name for a function that gives you probabilities.
        </p>

        <p>
          You can associate a value with the distribution \(p\) called <i>entropy</i>. It tells you how many of the outcomes are "reasonably likely". If only one outcome is likely, the entropy is low. If many are likely, it's high.
        </p>

        <p>
          I'm going to show you the formula for entropy and then try to make sense of it.
        </p>

        <p>
          Here it is...

          $$
          E(p) = - \sum_{i=1}^{i=N} p(i) \log p(i)
          $$
        </p>

        <p>
          First of all, why is there a minus sign in this equation? Well, each term in the sum is a multiplication of two parts: \(p(i)\) and \(\log p(i)\). \(p(i)\) is always between \(0\) and \(1\) and \(\log p(i)\) is always negative because \(\log x < 0\) when \(x < 1\). So the minus sign in front makes entropy is a positive number.
        </p>

        <p>
          Next, let's find the entropy for some simple distributions.
        </p>

        <p>
          Suppose outcome \(1\) has probability \(1\) and all other outcomes have probability \(0\). Then the entropy will be \(0\). Why? Because when \(p(i) = 0\), the first term in the sum is \(0\). And when \(p(i) = 1\), the second term is \(0\) (because \(\log 1 = 0\)). So the whole sum is \(0\).
        </p>

        <p>
          Suppose all outcomes have equal probability. Then \(p(i) = \dfrac{1}{N}\) for each outcome. The sum becomes...

          $$
          \begin{aligned}
          E(p) & = - \sum_{i=1}^{i=N}\dfrac{1}{N} \log \dfrac{1}{N} \\
               & = - \log \dfrac{1}{N} \\
               & = \log N
          \end{aligned}
          $$

          In words: when all \(N\) outcomes have equal probability, the entropy is \(\log N\). This is the maximum possible entropy for a set of \(N\) outcomes.
        </p>

        <p>
          The demo below lets you play with a simple probability distribution and see its entropy. You can drag the gray handles to move a bar up or down.
        </p>

      </div>
      <div class="bars-container">
      </div>
      <div id="entropy-label" class="entropy-label sans-serif">
      </div>

      <div class="demo-description">
        <p>
          Something tricky to be aware of: the sum of probabilities of all outcomes should equal \(1\). When you move a bar around, the demo will shift the probabilities of other bars so that they all sum to \(1\).
        </p>
      </div>
    </div>

    <script>
      renderMathInElement(document.body);
    </script>
  </body>
</html>
