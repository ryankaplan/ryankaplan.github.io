<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <!-- From here: http://stackoverflow.com/a/4389976 -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
    <title>Entropy Demo</title>
    <script src="entropy-demo.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" integrity="sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv+uR2SE/mbQcG7ATL28aI9H0" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js" integrity="sha384-/y1Nn9+QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js" integrity="sha384-dq1/gEHSxPZQ7DdrM82ID4YVol9BYyU7GbWlIwnwyPzotpoc57wDw/guX8EaYGPx" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="style.css" type="text/css">
  </head>
  <body>
    <div class="demo-container">
      <div class="demo-description">
        <h2>
          Entropy Demo
        </h2>
        <p>
          I've read many articles online with differing, vague, descriptions of entropy. But I recently found out that it has a clear mathematical representation!
        </p>

        <p>
          Suppose you're performing an experiment which has 5 possible outcomes. We'll label them \(i = {1, ..., 5}\). And suppose that they each occur with probability \(p(i)\). You can associate a value with the function \(p\) called entropy. It tells you how many of the outcomes are "reasonably likely". If only one outcome is likely, the entropy is low. If many are likely, it's high.
        </p>

        <p>
          The formula for entropy is as follows...

          $$
          E(p) = - \sum_{i=1}^{i=N} p(i) \log p(i)
          $$
        </p>

        <p>
          Let's make sense of this...
        </p>

        <p>
          First of all, why is there a minus sign in this equation? It's because \(\log x\) is negative when \(x < 0\). Since \(0 < p(i) < 1\), the entropy is a sum of negative values. The minus sign in front is so that entropy is a positive number.
        </p>

        <p>
          Suppose outcome \(1\) has probability \(1\) and all other outcomes have probability \(0\). Then the entropy will be \(0\). Why? Because when \(p(i) = 0\), the first term in the sum is \(0\). And when \(p(i) = 1\), the second term is \(0\) (because \(\log 1 = 0\)). So the whole sum is \(0\).
        </p>

        <p>
          Suppose all outcomes have equal probability. Then \(p(i) = \dfrac{1}{N}\) for each outcome. The sum becomes...

          $$
          \begin{aligned}
          E(p) & = - \sum_{i=1}^{i=N}\dfrac{1}{N} \log \dfrac{1}{N} \\
               & = - \log \dfrac{1}{N} \\
               & = \log N
          \end{aligned}
          $$

          In words: when all \(N\) outcomes have equal probability, the entropy is \(\log N\). This is the maximum possible entropy for a set of \(N\) outcomes.
        </p>

        <p>
          The demo below lets you play with a simple probability distribution and see its entropy. You can drag the gray handles to move a bar up or down.
        </p>

      </div>
      <div class="bars-container">
      </div>
      <div id="entropy-label" class="entropy-label sans-serif">
      </div>

      <div class="demo-description">
        <p>
          Something tricky to be aware of: the sum of probabilities of all outcomes should equal \(1\). When you move a bar around, the demo will shift the probabilities of other bars so that they all sum to \(1\).
        </p>
      </div>
    </div>

    <script>
      renderMathInElement(document.body);
    </script>
  </body>
</html>